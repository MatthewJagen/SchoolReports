{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw6nK0sAIW96"
   },
   "source": [
    "#### This assignment may be worked individually or in pairs. \n",
    "#### Enter your name(s) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AKeJiOaSIW9_"
   },
   "outputs": [],
   "source": [
    "#name(s) here\n",
    "# Matthew Jagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "oOQ1NW7eIW-B"
   },
   "source": [
    "# Assignment 3: KNN and NB classifiers\n",
    "\n",
    "In this assignment you'll implement the K-Nearest Neighbors (KNN) classifier to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignment on decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WSK7lm1vIW-B"
   },
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4B1oxNFIW-C"
   },
   "source": [
    "Q0. Read the data from a CSV file and store it in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pltM0J1KIW-C"
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    #your code goes here\n",
    "    data = pd.read_csv(filename, names=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\"])\n",
    "    return data\n",
    "\n",
    "# print(get_data(\"messidor_features.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wga5a1BkIW-D"
   },
   "source": [
    "You may use the following function to print a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6iuf-SzhIW-E"
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(TP, FN, FP, TN):\n",
    "    table_data = [[TP,FN],[FP,TN]]\n",
    "    df = pd.DataFrame(table_data, columns =['Predicted 1','Predicted 0'])\n",
    "    df = df.rename(index={0: 'Actual 1', 1: 'Actual 0'})\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpcTXeavIW-F"
   },
   "source": [
    "\n",
    "## K Nearest Neighbor (KNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex43a-Q2IW-G"
   },
   "source": [
    "Q1. Normalize the data so that each feature value lies between `[0-1]`.\n",
    "\n",
    "In class, we talked about why scaling the data is critical to KNN. We also talked about how data scaling should be done *inside the cross validataion loop*. This means that the scaling parameters should be based on the **training set only**, in order to prevent data leakage. Then the test data will need to be scaled, using the parameters found on the **training** data.\n",
    "\n",
    "Fill in the function to take in a training dataset and a test dataset and normalize them correctly. Return the normalized datasets.\n",
    "\n",
    "Caution: Return NEW datasets that have been normalized - do not normalize the datasets in-place, so that this can be run numerous times without altering the original data or normalizing already normalized data.\n",
    "\n",
    "Hint: When using dataframes, you can do this without a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RAsjuegyIW-H"
   },
   "outputs": [],
   "source": [
    "def normalize_data(train, test):\n",
    "    # your code goes here\n",
    "    train_norm = train.copy()\n",
    "    test_norm = test.copy()\n",
    "    \n",
    "    cols = train_norm.columns\n",
    "    for col in cols:\n",
    "# find the values needed for scaling this column using only training data\n",
    "        min_val = train_norm[col][train_norm[col].idxmin]\n",
    "        max_val = train_norm[col][train_norm[col].idxmax]\n",
    "        val_range = max_val-min_val\n",
    "        \n",
    "# normalize this column in training data using training data scalar values\n",
    "        train_norm[col] = train_norm[col] - min_val\n",
    "        train_norm[col] = train_norm[col] / val_range\n",
    "# scale this column in test data using training data scalar values\n",
    "        test_norm[col] = test_norm[col] - min_val\n",
    "        test_norm[col] = test_norm[col] / val_range\n",
    "    \n",
    "    return train_norm, test_norm\n",
    "\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# scaled_data, scaled_data2 = normalize_data(data, data)\n",
    "# print(scaled_data2.head(10))\n",
    "# print(scaled_data.head(10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JryybjV-IW-H"
   },
   "source": [
    "Q2. The distance calculation method is central to the KNN algorithm. In this assignment you'll be using Euclidean distance. \n",
    "\n",
    "Implement a function that takes in one data point (as a list), and the training data (as a dataframe), and calculates the Euclidian distance from the single data point to each of the data points in the training data.\n",
    "\n",
    "You may return these distances however you want (or may add them to the dataframe).\n",
    "\n",
    "Hint: \n",
    "For KNN, the distance calculations are the most time-consuming part of the algorithm. Even though computing Euclidian distance seems like a simple, and therefore quick, calculation, running it thousands of times, inside of a nested 5-fold cross-validation for example, can cause this algorithm to take a very long time to run, depending on your implementation. \n",
    "\n",
    "Remember, you almost never need to loop a Dataframe! Pandas DataFrames have been specifically optimized for fast operations on large datasets, by [vectorizing](https://www.quantifisolutions.com/vectorization-part-2-why-and-what) calculations across all rows at once.\n",
    "\n",
    "If you use a DataFrame, you should not write a loop to calculate each of the Euclidian distances one at a time. Look at [this post](https://stackoverflow.com/questions/46908388/find-euclidean-distance-from-a-point-to-rows-in-pandas-dataframe?rq=1) for more info.\n",
    "\n",
    "Caution: Be careful not to use the label in your distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fk3OHTdfIW-I"
   },
   "outputs": [],
   "source": [
    "def get_distances(point, df):\n",
    "    # your code goes here\n",
    "#     adapted from the stackoverflow post linked in Q2\n",
    "    distances = df[df.columns[0:-1]].sub(np.array(point[0:-1])).pow(2).sum(1).pow(0.5)\n",
    "    return distances\n",
    "\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# print(get_distances(data.iloc[0], data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUc3HUrEIW-I"
   },
   "source": [
    "Q3. Build your KNN classifier.\n",
    "\n",
    "This function takes in a training set (as a dataframe), a test set (as a dataframe), and a k to use, and classifies all data points in the test set, using the data in the training set and the given k.\n",
    "\n",
    "It should return the predicted labels for the test set as a list.\n",
    "\n",
    "Caution: Remember to normalize your data before doing distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WW4LZocWIW-J"
   },
   "outputs": [],
   "source": [
    "def run_knn(train_set, test_set, k):\n",
    "    # your code goes here\n",
    "#   normalize datasets with training data\n",
    "    train_norm, test_norm = normalize_data(train_set, test_set)\n",
    "    \n",
    "    predictions = []\n",
    "#   loop to predict points in training data\n",
    "    for i in test_norm.index:\n",
    "        point = test_norm.iloc[i].tolist()\n",
    "        \n",
    "#       find test point's K nearest neighbors in the training data\n",
    "        distances = get_distances(point, train_norm)\n",
    "        distances.sort_values(inplace=True)\n",
    "        near_neighbors = distances[0:k]\n",
    "        \n",
    "#       use the neighbors to predict the class label\n",
    "        yes_count = 0\n",
    "        for i in near_neighbors.index:\n",
    "            yes_count += train_norm[\"19\"][i]\n",
    "        prediction = 0\n",
    "        if yes_count/k >= 0.5: #if for some reason K is even, bias 50/50 to positive\n",
    "            prediction = 1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# print(run_knn(data.copy(), data.copy(), 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CTYxjDTIW-J"
   },
   "source": [
    "Q4. Find the best value of k for this data. \n",
    "\n",
    "Try k ranging from 1 to 10 (odds only). For each k value, use a 5-fold cross validation to evaluate the accuracy with that k. In each fold of CV, divide your data into a training set and a validation set. Print out the best value of k and the accuracy achieved with that value. Return the best value of k. If there is a tie for best k, use the lowest of the k values.\n",
    "\n",
    "Hint: This is the *inner* loop of a nested cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i1xsDAupIW-K"
   },
   "outputs": [],
   "source": [
    "def find_best_k(data):\n",
    "    # your code goes here   \n",
    "    num_folds = 5 \n",
    "    k_values = [1, 3, 5, 7, 9]\n",
    "    k_accuracies = []\n",
    "    for k in k_values:\n",
    "        accuracies = []\n",
    "        for i in range(num_folds):\n",
    "#           split data into test set (from partition start to partition end) and training set (everything else)\n",
    "            partition_start_idx = int((i)*(len(data)/num_folds))\n",
    "            partition_end_idx = int((i+1)*(len(data)/num_folds))\n",
    "            test_set = data.iloc[partition_start_idx:partition_end_idx]\n",
    "            train_set_first_half = data.iloc[:partition_start_idx]\n",
    "            train_set_second_half = data.iloc[partition_end_idx:]\n",
    "            train_set = pd.concat([train_set_first_half, train_set_second_half])\n",
    "            test_set.reset_index(inplace=True)\n",
    "            train_set.reset_index(inplace=True)\n",
    "            \n",
    "#           use run_knn to get predictions on the test set from the training set and then calculate the accuracy of this fold\n",
    "            predictions = run_knn(train_set, test_set, k)\n",
    "            num_correct = 0\n",
    "            for i in test_set.index:\n",
    "                if test_set[\"19\"][i] == predictions[i]:\n",
    "                    num_correct += 1\n",
    "            accuracies.append(num_correct/len(predictions))\n",
    "            \n",
    "#       add the average accuracy for this k value to the k_accuracies list\n",
    "        total = 0.0\n",
    "        for accuracy in accuracies:\n",
    "            total += accuracy\n",
    "        k_accuracies.append(total/len(accuracies))\n",
    "        \n",
    "#   find the k value with the highest average accuracy\n",
    "    max_acc_index = 0\n",
    "    for i in range(len(k_accuracies)):\n",
    "        if k_accuracies[i] > k_accuracies[max_acc_index]:\n",
    "            max_acc_index = i\n",
    "    best_k = k_values[max_acc_index]\n",
    "    return best_k\n",
    "\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# print(find_best_k(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2bap23KIW-K"
   },
   "source": [
    "Q5. Now measure the accuracy of your classifier using 5-fold cross validation. \n",
    "\n",
    "In each fold of this CV, divide your data into a training set and a test set. The training set should get sent through your code for Q4, resulting in a value of k to use. Using that k, calculate an accuracy on the test set. You will average the accuracy over all 5 folds to obtain the final accuracy measurement. \n",
    "\n",
    "Print the accuracy, the confusion matrix, and the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8U04SRmQIW-K",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6156521739130435\n",
      "Precision (+):  0.6556169429097606\n",
      "Recall (+):  0.5826513911620295\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>356</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>187</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 0\n",
       "Actual 1          356          255\n",
       "Actual 0          187          352"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time: 37.9044885635376\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data = get_data('messidor_features.txt')\n",
    "start_time = time.time()\n",
    "\n",
    "# your code goes here\n",
    "num_folds = 5\n",
    "accuracies = []\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "for i in range(num_folds):\n",
    "#   split data into test set (from partition start to partition end) and training set (everything else)\n",
    "    partition_start_idx = int((i)*(len(data)/num_folds))\n",
    "    partition_end_idx = int((i+1)*(len(data)/num_folds))\n",
    "    test_set = data.iloc[partition_start_idx:partition_end_idx]\n",
    "    train_set_first_half = data.iloc[:partition_start_idx]\n",
    "    train_set_second_half = data.iloc[partition_end_idx:]\n",
    "    train_set = pd.concat([train_set_first_half, train_set_second_half])\n",
    "    test_set.reset_index(inplace=True)\n",
    "    train_set.reset_index(inplace=True)\n",
    "    \n",
    "#   find best k and use it to get the accuracy for this fold (also increment counts for our confusion matrix)\n",
    "    best_k = find_best_k(train_set)\n",
    "    predictions = run_knn(train_set, test_set, best_k)\n",
    "    num_correct = 0\n",
    "    j = 0\n",
    "    for label in test_set[\"19\"]:\n",
    "        if label == predictions[j]:\n",
    "            num_correct += 1\n",
    "            if predictions[j] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if predictions[j] == 1:\n",
    "                FP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        j += 1\n",
    "    \n",
    "    accuracies.append(num_correct/len(predictions))\n",
    "\n",
    "# calculate the final accuracy measurement\n",
    "total = 0.0\n",
    "for accuracy in accuracies:\n",
    "    total += accuracy\n",
    "accuracy = total/len(accuracies)\n",
    "\n",
    "# print out the accuracy, precision, recall, and confusion matrix\n",
    "print(\"Accuracy: \", str(accuracy))\n",
    "precision = TP/(TP+FP)\n",
    "print(\"Precision (+): \", str(precision))\n",
    "recall = TP/(TP+FN)\n",
    "print(\"Recall (+): \", str(recall))\n",
    "print_confusion_matrix(TP, FN, FP, TN)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('\\nTotal time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3_KNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
